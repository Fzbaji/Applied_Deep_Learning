# Applied Deep Learning - Parcours Complet

## Vue d'ensemble

Ce repository constitue un **parcours p√©dagogique complet** couvrant les architectures fondamentales du Deep Learning, de la r√©gression simple aux r√©seaux de neurones avanc√©s (CNN, RNN, LSTM), avec applications pratiques sur des probl√©matiques vari√©es : classification d'images, analyse de s√©ries temporelles et traitement du langage naturel.

---

## üìÇ Structure 

### **Livrable 1 : R√©gression Lin√©aire Multi-Approches**
üìÑ `Livable_1_RL_from_Scratch_SickitLearn_TensorFlow.ipynb`

Comparaison de trois impl√©mentations de r√©gression lin√©aire :
- **From Scratch** : Impl√©mentation manuelle avec NumPy (gradient descent)
- **Scikit-learn** : API haut niveau pour Machine Learning
- **TensorFlow/Keras** : Framework Deep Learning

**Objectifs :** Comprendre les fondamentaux de l'optimisation et la descente de gradient

---

### **Livrable 2 : R√©seaux de Neurones - Fonctions d'Activation**
üìÑ `Livrable_2_RNA_(RL+ReLu_Sigmoid).ipynb`

√âtude comparative des fonctions d'activation ReLU vs Sigmoid :
- Exp√©rience 1 : Donn√©es lin√©airement s√©parables
- Exp√©rience 2 : Donn√©es non lin√©airement s√©parables
- Exp√©rience 3 : Probl√®mes complexes avec bruit

**Concepts cl√©s :** Vanishing gradient, convergence, capacit√© de mod√©lisation non-lin√©aire

---

### **Livrable 3 : CNN - Fondamentaux**
üìÑ `Livrable_3_CNN_solo.ipynb`

Introduction aux r√©seaux de neurones convolutifs :
- Op√©rations de convolution (stride, padding)
- Couches de pooling (max pooling, average pooling)
- Extraction de features visuelles

**Focus :** Compr√©hension des m√©canismes de base des CNN

---

### **Livrable 4 : CNN - Application MNIST**
üìÑ `Livrable_4_CNN_Mnist_Dataset.ipynb`

Comparaison de **5 architectures CNN** sur deux datasets :
- **MNIST** : Chiffres manuscrits (0-9)
- **Fashion-MNIST** : V√™tements et accessoires

**Architectures test√©es :**
1. CNN Simple (2 conv + 1 dense)
2. CNN avec Dropout
3. CNN avec Batch Normalization
4. CNN Profond (4 couches conv)
5. CNN avec Data Augmentation

**M√©triques :** Accuracy, loss, matrices de confusion, courbes d'apprentissage

---

### **Livrable 5 : S√©ries Temporelles**
üìÅ `Livrable_5_Time_Series/`

#### **S√©ance 1 : Mod√®les Statistiques et Optimisation**
üìÑ `Time_Series_S√©ance1.ipynb`

- G√©n√©ration de s√©ries synth√©tiques (tendance + saisonnalit√© + bruit)
- Tests de stationnarit√© (ADF)
- Autocorr√©lation (ACF/PACF)
- **Mod√®les ARIMA et SARIMA** avec param√®tres explicit√©s
- **R√©gression avec feature engineering** (lags, dummies saisonniers)
- **Grid Search automatique** : Optimisation sur 144 configurations SARIMA
- Comparaison des performances (MSE, MAE)

#### **S√©ance 2 : Comparaison Multi-Mod√®les**
üìÑ `Time_Series_S√©ance2_models_comparision.ipynb`

√âvaluation comparative de diff√©rentes approches de pr√©vision

#### **M5 Forecasting : Application R√©elle**
üìÑ `M5_Forecasting_NN_Comparison.ipynb`

- Dataset Kaggle M5 (ventes Walmart)
- Preprocessing avanc√©
- Comparaison d'architectures de r√©seaux de neurones
- M√©triques de performance sur donn√©es r√©elles

---

### **Livrable 6 : NLP - D√©tection de Sarcasme**
üìÑ `Livrable_6_Sarcasm_NLP.ipynb`

Analyse de sentiment avec **7 architectures NLP** :

1. **Baseline** : R√©gression Logistique (TF-IDF)
2. **Simple RNN**
3. **LSTM** (Long Short-Term Memory)
4. **Bi-LSTM** (Bidirectionnel)
5. **GRU** (Gated Recurrent Unit)
6. **CNN 1D** pour texte
7. **Hybrid CNN-LSTM**

**Pipeline complet :**
- Tokenization et padding
- Embeddings Word2Vec/GloVe
- Architectures r√©currentes vs convolutives
- √âvaluation comparative (accuracy, F1-score, confusion matrices)

---

## Objectifs P√©dagogiques

### **Comp√©tences Techniques**
‚úÖ Impl√©mentation from scratch et utilisation de frameworks (TensorFlow/Keras)  
‚úÖ Comparaison rigoureuse d'architectures (baseline vs avanc√©es)  
‚úÖ Feature engineering et preprocessing adapt√© √† chaque domaine  
‚úÖ Optimisation d'hyperparam√®tres (grid search, validation)  
‚úÖ √âvaluation multi-m√©triques et visualisations

### **Domaines Couverts**
- **R√©gression** : Lin√©aire et polynomiale
- **Classification** : Images (CNN), texte (NLP)
- **S√©ries Temporelles** : ARIMA/SARIMA, r√©seaux de neurones
- **NLP** : Embeddings, RNN, LSTM, attention mechanisms

### **Progression Structur√©e**
1. **Fondamentaux** : R√©gression, perceptron, activations
2. **Vision** : Convolutions, architectures CNN
3. **S√©quences** : RNN, LSTM, s√©ries temporelles
4. **Langage** : Embeddings, traitement de texte

---

## Technologies Utilis√©es

| Framework | Usage |
|-----------|-------|
| **NumPy** | Calculs num√©riques, impl√©mentations from scratch |
| **Pandas** | Manipulation de donn√©es tabulaires |
| **Matplotlib/Seaborn** | Visualisations |
| **Scikit-learn** | Machine Learning classique, m√©triques |
| **TensorFlow/Keras** | Deep Learning, architectures neuronales |
| **Statsmodels** | Mod√®les statistiques (ARIMA/SARIMA) |
| **NLTK/SpaCy** | Preprocessing NLP |

---

## M√©thodologie

Chaque livrable suit une structure coh√©rente :

1. **Introduction th√©orique** : Concepts et enjeux
2. **Exploration des donn√©es** : Statistiques, visualisations
3. **Preprocessing** : Normalisation, feature engineering
4. **Mod√©lisation** : Impl√©mentation et entra√Ænement
5. **√âvaluation** : M√©triques multiples, comparaisons
6. **Interpr√©tation** : Analyse des r√©sultats, recommandations


---

## üìñ Comment Utiliser ce Repository

### **Pour Apprendre**
Suivre l'ordre des livrables (1 ‚Üí 6) pour une progression logique

### **Pour Appliquer**
Adapter les architectures et pipelines √† vos propres datasets
